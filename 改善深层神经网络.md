# 改善深层神经网络

> 训练验证集应该和测试集来自于同一分布  

## 偏差和方差 bias / variance
> 贝叶斯误差，最优误差  
> 方差过大，过拟合，解决方案：增加数据或者正则化  

### 正则化 regularization(L2正则)
> Logistic regression regularization  
> $J(w,b)=\frac{1}{m} \sum_{i=1}^m cost(\hat{y}^{i},y^{i})+\frac{\lambda}{2m}||w||^2$ \  
> neural network
> $J(w^{[1]},b^{[1]},...,w^{[L]},b^{[L]})=\frac{1}{m} \sum_{i=1}^m cost(\hat{y}^{i},y^{i})+\frac{\lambda}{2m}\sum_{l=1}^L||w||_F^2$ \
>字母Ｌ是神经网络所含的层数，$||W||^{2}为范数平方，被定义为矩阵中所有元素的平方求和 \

>$||w^{[l]}||_F^{2}=\sum_{i=1}^{n[l-1]}\sum_{j=1}^{n[l]}(w_{ij}^{[l]})^{2}$ \
> $dw^{[l]}=(from backpop)+\frac{\lambda}{m}w^{[l]}$

##　为什么正则化可以减少过拟化