# 改善深层神经网络

> 训练验证集应该和测试集来自于同一分布  

## 偏差和方差 bias / variance
> 贝叶斯误差，最优误差  
> 方差过大，过拟合，解决方案：增加数据或者正则化  

### 正则化 regularization(L2正则)
> Logistic regression regularization  
> $J(w,b)=\frac{1}{m} \sum_{i=1}^m cost(\hat{y}^{i},y^{i})+\frac{\lambda}{2m}||w||^2$ \  
> neural network
> $J(w^{[1]},b^{[1]},...,w^{[L]},b^{[L]})=\frac{1}{m} \sum_{i=1}^m cost(\hat{y}^{i},y^{i})+\frac{\lambda}{2m}\sum_{l=1}^L||w||_F^2$ \
>字母Ｌ是神经网络所含的层数，$||W||^{2}为范数平方，被定义为矩阵中所有元素的平方求和 \

>$||w^{[l]}||_F^{2}=\sum_{i=1}^{n[l-1]}\sum_{j=1}^{n[l]}(w_{ij}^{[l]})^{2}$ \
> $dw^{[l]}=(from backpop)+\frac{\lambda}{m}w^{[l]}$

###　为什么正则化可以减少过拟化

直观上理解就是如果正则化$\lambda$设置得足够大，权重矩阵Ｗ被设置为接近于0的值，直观理解就是把多隐藏单元的权重设为0，于是基本上消除了这些隐藏单元的许多影响。如果是这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会使这个网络从过度拟合的状态更接近左图的高偏差状态。

但是会存在$\lambda$一个中间值，于是会有一个接近“Just Right”的中间状态。


### dropout regularization (适合计算机视觉)
####　inverted dropout 反向随机失活
>此处以三隐层网络为例，keep-prob等于0.8,消除任意一个隐藏单元的概率为0.2 \
>$d3=np.random.rand(a3.shape[0],a3.shape[1])<keepprob$ \
>a3=np.multiply(a3,d3) \
>a3 /= keep-prob(dropout 方法)
#### backward propagation with dropout
>eg.. dA3=dA3*D3 (D1是之前前向传播的dropoutD3)\
>     dA3=dA3/keep_prob

#### 理解dropout
>dropout一大缺点就是代价函数J不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。定义明确的代价函数J每次迭代后都会下降，因为我们所优化的代价函数J实际上并没有明确定义，或者说在某种程度上很难计算，所以我们失去了调试工具来绘制这样的图片。我通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug


### other regularization
>early stopping 代表提早停止训练神经网络 \
>当你还未在神经网络上运行太多迭代过程的时候，参数w接近0，因为随机初始化值时，它的值可能都是较小的随机值，所以在你长期训练神经网络之前依然很小，在迭代过程和训练过程中的值会变得越来越大，比如在这儿，神经网络中参数w的值已经非常大了，所以early stopping要做就是在中间点停止迭代过程，我们得到一个w值中等大小的弗罗贝尼乌斯范数 \
early stopping的主要缺点就是你不能独立地处理这两个问题，因为提早停止梯度下降，也就是停止了优化代价函数，因为现在你不再尝试降低代价函数，所以代价函数的值可能不够小，同时你又希望不出现过拟合，你没有采取不同的方式来解决这两个问题，而是用一种方法同时解决两个问题

## 归一化输入 normalize input
>第一步是零均值化 $\mu = \frac{1}{m} \sum_{i=1}^mx^{(i)}  x=x-\mu$ \
$x=x-\mu$ \
>第二步是归一化方差 $\sigma^2=\frac{1}{m}\sum_{i=1}^m(x^{(i)})^2$ \
> $x=x/\sigma^2$ \
>当输入参数在非常不同的取值范围内，如其中一个从1到1000，另一个从0到1，这对优化算法非常不利。但是仅将它们设置为均化零值，假设方差为1，就像上一张幻灯片里设定的那样，确保所有特征都在相似范围内，通常可以帮助学习算法运行得更快。

## 梯度消失或梯度爆炸 vanishing/exploding gradients
>深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小

## 神经网络权重初始化
>$w^{[l]}=np.random.randn(shape)*np.sqrt(\frac{1}{n^{[l-1]}})$ 

## 梯度检验
### 梯度的数值逼近
>单边公差 \

![image](http://www.ai-start.com/dl2017/images/91cde35a0fc9c11a98f16ad2797f20a7.png)
>双边公差 \

![image](http://www.ai-start.com/dl2017/images/b24ad9ce9fdc4a32be4d98915d8f6ffb.png)

>$\frac{f(\theta+\epsilon)-f(\Theta-\epsilon)}{2\epsilon}\approx g(\theta)$

### 梯度检验 grad check
>为了执行梯度检验，首先要做的就是，把所有参数转换成一个巨大的向量数据 \
>你要做的就是把矩阵W转换成一个向量，把所有矩阵转换成向量之后，做连接运算，得到一个巨型向量$\theta$ \
>同样的，把$dW^{[1]}$转换成矩阵，$db^{[1]}$已经是一个向量了，直到把$dW^{[1]}$转换成矩阵，这样所有的dW都已经是矩阵，注意与$W^{[1]}$具有相同维度,经过相同的转换和连接运算操作之后，你可以把所有导数转换成一个大向量d$\theta$

>for each i:
  $d\theta appox[i]=\frac{J(\theta_1...,\theta_{i+\epsilon},..)-J(\theta_1...,\theta_{i-\epsilon},..)}{2\epsilon}$

check  if $d \theta appox \approx d\theta$ with :
$temp=\frac{||d\theta appox - d\theta||_2}{||d\theta appox||_2 + ||d\theta||_2}$\
if $temp \approx 10^{-7}$ 很大几率没有bug \
else if$temp \approx 10^{-3}$ 很大几率会出现bug\
不要在训练中使用梯度检验,只在调试的时候进行