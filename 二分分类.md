# 神经网络基础

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
## binary classification 二分分类
$$(x,y)$$
$$ m=num\ of\ training\ example$$
$$x\in \R^{nx},y\in\{0,1\}$$
$$
X = [x^{1},....,x^{m}]
$$
$$ X \in \R^{nx*m} $$
$$  Y=[y^{1},...,y^{m}]$$
$$ Y \in \R^{1*m}$$ 
![avatar](/home/lin/桌面/nlp/神经网络/image/FireShot Capture 19 - 神经网络和深度学习 - 网易云课堂_ - https___mooc.study.163.com_learn_2.png)

## logistic regression 逻辑回归

>given x ,want $\hat{y}$=P(y=1|x) \
$x \in R^{nx}$ ,$0 \leq \hat{y}\leq 1$ \
parameters $w \in R^{nx},b \in R$ \
output $\hat{y}=\sigma(w^{T}x+b)$ \
sigmoid(z)函数:　$\sigma(z)=\frac{1}{1-e^-z}$

![sigmoid函数](https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/w%3D268%3Bg%3D0/sign=ba0ac7a864061d957d46303e43cf6dec/d009b3de9c82d158dfb4e7218a0a19d8bc3e426f.jpg)

## logistic regression cost function 代价函数
>衡量ｗ,b在训练集中的效果　\
训练参数w 和　ｂ \
given {$(x^{1},y^{1}),..,(x^{m},y^{m})$},want $\hat{y}\approx y^{i}$ \
loss/error function(单个样本): \
$error(\hat{y},y)=\frac{1}{2}(\hat{y}-y)^{2}$ 平方差　是非凸的　不采用 \
$error(\hat{y},y)=-(y\log(\hat{y})+(1-y)\log(1-\hat{y}))$ 是凸函数　采用这个　\
if y=1:$error(\hat{y},y)=-log(\hat{y}) \impliedby (want\ cost\ small \to want\ \hat{y}\ large(\hat{y}\to1))$ \
if y=1:$error(\hat{y},y)=-log(1-\hat{y}) \impliedby (want\ cost\ small \to want\ \hat{y}\ small(\hat{y}\to0))$ 

>cost function(全体样本): \
$J(w,b)=\frac{1}{m}\sum_{i=1}^{m}{error(\hat{y}^{i},y^{i})}
        =-\frac{1}{m}\sum_{i=1}^{m}{(y^{i}\log(\hat{y}^{i})+(1-y^{i})\log(1-\hat{y}^{i}))}$ 

## gradient descent 梯度下降
>want to find w,b that minimize J(w,b) \
Reapeat { \
$w:=w-\alpha\frac{\partial J(w,b)}{\partial w}$ \
$b:=b-\alpha\frac{\partial J(w,b)}{\partial b}$ \
}

### 向后传播　求导　Ｊ(w,b)
#### 求导loss/error function
>$\fbox{z=w1x1+w2x2+b} \to$
$\fbox{a=σ(z)   } \to$
$\fbox{error(a,y)}$ \
\
$'da'=\frac{derror(a,y)}{da}=-\frac{y}{a}+\frac{1-y}{1-a}$ \
$'dz'=\frac{derror(a,y)}{da}*\frac{da}{dz} = a-y$ \
$'dw1'=\frac{derror(a,y)}{da}*\frac{da}{dz}*\frac{dz}{dw1} = x1*'dz'$

#### gradient on m examples 求导　Ｊ(w,b)
$J(w,b)=\frac{1}{m}\sum_{i=1}^{m}{error(\hat{y}^{i},y^{i})}
        =-\frac{1}{m}\sum_{i=1}^{m}{(y^{i}\log(\hat{y}^{i})+(1-y^{i})\log(1-\hat{y}^{i}))}$ 

>e.g. \
J=0;'dw1'=0;'dw2'=0;'db'=0 \
for i=1 to m: \
$z^{i}=w^{T}x^{i}+b$ \
$a^{i}=\sigma{z^{i}}$ \
$J += -(y\log(\hat{y})+(1-y)\log(1-\hat{y}))$ \
$dz^{i}=a^{i}-y^{i}$ \ 
$dw1 += x1^{i}dz^{1}$ \
$dw2 += x2^{i}dz^{1}$ \
$db += dz^{i}$ \
end for \
J = J/m \
dw1 = dw1/m , dw2 = dw2/m , db = db/m \
$w1:=w1-\alpha dw1$ \
$w2:=w2-\alpha dw2$ \
$b:=b-\alpha db$ 
