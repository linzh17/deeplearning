# 浅层神经网络

## 神经网络表示　neural network Representation

### 双层神经网络
* input layer $a^{[0]}=x$
* hidden layer $a^{[1]}$
* output layer $a^{2} \hat{y}=a^{[i]}$
>计算神经网络的层数，不包含输入层　\
计算过程　\
given input x: \
$z^{[1]}=W^{[1]}+b^{[1]}$ \
$a^{[1]}=\sigma (z^{[1]})$ \
$z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$ \
$a^{[2]}=\sigma (a^{[2]})$ \
计算过程图　可以看补充课件　神经网络计算过程

### vectorizing across multiple examples (m个训练样本的向量化)
以双层神经网络为例\
$X \in R^{nx*m}$ \
$Z^{[1]}=W^{[1]}X+b^{[1]}$ \
$A^{[1]}= \sigma (Z^{[1]})$ \
$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$ \
$A^{[2]}=\sigma (Z^{[2]})$ 

### 激活函数
>tanh函数或者双曲正切函数是总体上都优于sigmoid函数的激活函数 \
tanh函数是sigmoid的向下平移和伸缩后的结果。对它进行了变形后，穿过了原点,并且值域介于+1和-1之间　\
果使用tanh函数代替sigmoid函数中心化数据，使得数据的平均值更接近0而不是0.5 \
在二分类问题上，对于输出层，因为ｙ的值是0或1,sigmoid函数更适合 \
sigmoid函数和tanh函数两者共同的缺点是，在z特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度 \
修正线性单元的函数（ReLu） 只要z是正值的情况下，导数恒等于1，当z是负值的时候，导数恒等于0 \
Leaky Relu 当z是负值时，这个函数的值不是等于0，而是轻微的倾斜 \

#### 为什么需要非线性激活函数
>简单的说，线性函数的组合还是线性函数，这样就等于没有了隐藏层，这样的神经网络和逻辑回归是等价的

#### 激活函数的导数
>$sigmoid函数　g(z)=\frac{1}{1+e^{z}};  g^{'}(z)=g(z)(1-g(z))$ \

>$tanh函数　g(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}; g^{'}(z)=1-(g(z))^{2}$ \

>$ReLU函数　g(z)=max(0,z);
 g^{'}(z)=
\begin{cases}
0, &\text{if z<0} \\
1, &\text{if z>0} \\
undefind,&\text{if z=0}
\end{cases}$ 


>$leky ReLU函数　g(z)=max(0.01z,z);
 g^{'}(z)=
\begin{cases}
0.01, &\text{if z<0} \\
1, &\text{if z>0} \\
undefind,&\text{if z=0}
\end{cases}$


### 神经网络的梯度下降的实现
>$dz^{2}=a^{[2]}-y$ \
$dW^{[2]}=dz^{[2]}a^{[1]^{T}}$ \
$db^{[2]}=dz^{[2]}$ \
$dz^{1}=W^{[2]^{T}}dz^{[2]}*g^{[1]^{'}}(z^{[1]})$ \
$dW^{[1]}=dz^{[1]}x^{T}$ \
$db^{[1]}=dz^{[1]}$ 

>向量化实现　\
$dZ^{[2]}=A^{[2]}-Y$ \
$dW^{[2]}=\frac{1}{m}dZ^{[2]}A^{[1]^{T}}$　\
$dv^{[2]}=\frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True)$ \
$dZ^{[１]}=W^{[2]^{T}}dZ^{[2]}*g^{[1]^{'}}(Z^{[1]})$ \
$dW^{[１]}=\frac{1}{m}dZ^{[1]}X^{T}$　\
$dv^{[１]}=\frac{1}{m}np.sum(dZ^{[1]},axis=1,keepdims=True)$ 

### 随机初始化
>随机初始化权重，如果神经网络中权重初始化为０，是无效的，因为每一层的激活函数中的各项单元值会一样，完全对称，那么各项隐藏单元对输出单元的影响也一样，这样多个隐藏单元将会没有意义，等价于一个隐藏单元\
b可以初始化为０

>以单隐层神经网络为例 \
W^{{1}}=np.random.randn(2,2)*0.01 \
为什么是0.01，而不是100或者1000。我们通常倾向于初始化为很小的随机数。因为如果你用tanh或者sigmoid激活函数，或者说只在输出层有一个Sigmoid，如果（数值）波动太大，当你计算激活值时,如果W很大，激活值ｚ就会很大或者很小，因此这种情况下你很可能停在tanh/sigmoid函数的平坦的地方这些地方梯度很小也就意味着梯度下降会很慢，因此学习也就很慢。




