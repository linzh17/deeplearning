a# 循环序列模型

## 循环神经网络模型

### 传统的神经网络模型  
> 在人名识别问题上出现的问题  
* 输入和输出的数据在不同的例子中为不同的长度 ，需要填充到相同的长度，才可以进行计算 
* 传统的神经网络不能共享从文本不同位置所学习到的特征 ，在文本输入为10000的向量时，需要大量的参数，而使用循环神经网络可以解决这个问题
  
### 循环神经网络
> 循环神经网络，在每一个时间步中，循环神经网咯会传递一个激活值到下一个时间步中，用于下一个时间步的计算
![image](https://img-blog.csdn.net/20180303161533313?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)  
在零时刻需要编造一个激活值，通常输入一个零向量，或者随机初始的向量

> 循环神经网络是从左到右扫描数据，同时共享每个时间步的参数  
> $W_{ax}管理输入x^{t}到隐藏蹭的连接，每个时间步都用相同的W_{ax}$;
> $W_{aa}$管理激活值$a^{<t>}$到隐藏层的连接
> $W_{ya}$管理隐藏层到激活值$y^{<t>}$的连接

* 目前 上述的循环神经网络缺点 ，每个预测输出只用了前面的输入信息，而没有使用后面的信息
* BRNN 双向循环神经网络可以解决这种缺点

#### 循环神经网络的前向传播 
* 构造初始激活向量 $a^{<0>} = \vec{0}$
* $a^{<1>} = g(W_{aa}a^{<0>}+W_{ax}x^{<1>}+b_a)$ (通常选择tanh 作为激活函数，有时候也使用relu函数，tanh梯度消失的问题可以用其他方式解决)
* $\hat{y}^{<1>} = g(W_{ya}a^{<1>}+b_y$ 如果是二分类问题，使用sigmoid作为激活函数，如果是多分类问题，使用softmax激活函数
* 最终 
  * $a^{<t>} = g(W_{aa}a^{<t-1>}+W_{ax}x^{<t>}+b_a)$
  * $\hat{y}^{<t>} = g(W_{ya}a^{<t>}+b_y)$
* 前向传播公式的简化
  * $a^{<t>} = g(W_{a}[a^{<t-1>},x^{<t>}]+b_a)$
  *  $\hat{y}^{<t>} = g(W_{y}a^{<t>}+b_y)$
* 其中
  * $W_a = [W_{aa}:W_{ax}] W_{aa}为(100,100)的矩阵
     W_{ax}是(100,10000)维的矩阵，W_a是(100,10100)的矩阵$
  *   $[a,x]= \begin{bmatrix}
      a^{t-1}\\x^{t}
        \end{bmatrix}$ 表示一个(10100)的向量
#### 穿越时间的反向传播 
> 为了进行反向传播计算，使用梯度下降等方法来更新RNN的参数，我们需要定义一个损失函数
* $L^{<t>}(\hat{y}^{<t>},y^{<t>}) = -y^{<t>}log
\hat{y}^{<t>}-(1-y^{<t>})log(1-\hat{y}^{<t>})$
*$L(\hat{y},y)=\sum^{T_y}_{t=1}L^{<t>}(\hat{y}^{<t>},y^{<t>})$ 
* 反向传播按照前向传播相反的方向进行导数计算，来对参数进行更新

### 不同类型的RNN 
* many-to-many($T_x=T_y$) 
  * > 这种情况下输入和输出的长度相同，用于分析句子是否含有人名或其他成分的例子
  * ![image](https://img-blog.csdn.net/20180303204128593?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* many-to-one 
  * > 在情感分析问题中，对某个序列进行正负判别或者打星评分，例如 输入一个序列，但只输出一个值
  * ![image](https://img-blog.csdn.net/20180303213623316?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* one-to-many
  * > 例如在音乐生成的例子中，输入一个音乐的类型或者空值，生成一段音乐序列或者音符序列，这种情况就是输入一个值，输出一个序列：
  * ![image](https://img-blog.csdn.net/20180303214120180?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* many-to-many($T_x \neq T_y$)
  * > 我们上面介绍的一种RNN的结构是输入和输出序列的长度是相同的，但是像机器翻译这种类似的应用来说，输入和输出都是序列，但长度却不相同，这是另外一种多对多的结构：
  * ![image](https://img-blog.csdn.net/20180303214555208?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### 语言模型和序列生成 
* 语言模型，从输入的句子中，评估各个单词出现的可能性，进而给出整个句子出现的可能性
* 使用RNN构建语言模型
  * 训练集：一个很大的语言文本语料库；
  * Tokenize：将句子使用字典库标记化；
  * 其中，未出现在字典库中的词使用“UNK”来表示；
  * 第一步：使用零向量对输出进行预测，即预测第一个单词是某个单词的可能性；
  * 第二步：通过前面的输入，逐步预测后面一个单词出现的概率；
  * 训练网络：使用softmax损失函数计算损失，对网络进行参数更新，提升语言模型的准确率。
  * 例子 计算句子（$y^1,y^2,y^3$）各个单词的概率相乘得句子的概率(y^1,y^2,y^3) = P(y^1)P(y^2|y^1)P(y^3|y^1,y^2)$ 

#### 新序列采样
> 在完成一个序列模型的训练之后，如果我们想要了解这个模型学到了什么，其中一种非正式的方法就是进行一次新序列采样（sample novel sequences）  
> 对一个序列模型，其模拟了任意特定单词序列的概率，而我们要做的就是对这个概率分布进行采样，来生成一个新的单词序列
>  对已经训练好的RNN模型，采样所要做的是  

* 首先输入$x^{<1>=0},a^{<0>}=0$，在这第一个时间步，我们得到所有可能的输出经过softmax层后可能的概率，根据这个softmax的分布，进行随机采样，获取第一个随机采样单词$y^{<1>}$；
* 然后继续下一个时间步，我们以刚刚采样得到的$y^{<1>}$作为下一个时间步的输入，进而softmax层会预测下一个输出$y^{<2>}$，依次类推；
* 如果字典中有结束的标志如：“EOS”，那么输出是该符号时则表示结束；若没有这种标志，则我们可以自行设置结束的时间步。

##### 基于词汇的字符模型
> 上述的模型是基于词汇的语言模型，我们可以构建基于字符的语言模型，每个单词和符号代表一个相应的输入或者输出
![image](https://img-blog.csdn.net/20180303223211083?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

* 基于字符的语言模型优缺点
  * 优点
    * 不会出现未知字符
  * 缺点
    * 最后会得到太多太长的输出序列
    * 对于句子前后部分依赖关系的捕捉没有词汇语言模型效果好
    * 基于字符的模型训练成本高

### RNN 的梯度消失
> RNN 存在一个缺陷，梯度消失  
> The cat, which already ate ………..，was full；  
The cats, which already ate ………..，were full.  
> 第一个句子cat 对应was ,第二个句子cats对应were ,句子中间存在很多单词，句子中存在这长期依赖，只前面的单词对后面的单词有着重要的影响，但是目前RNN 模型，不擅长捕获这种长期依赖关系  
![image](https://img-blog.csdn.net/20180303225107246?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
> 如上图所示，深度的RNN 网络，输出的y得到的梯度很难通过反向传播传播回去，很难对前面的权重产生影响，意味着很难让网络记住前面的单词的特征来对后面的输出进行影响

> 梯度爆炸也会在RNN中出现，但他会影响参数变得很大，出现NAN 或者参数溢出，所以很容易发现，可以通过梯度修剪来解决这个问题 但是梯度消失，比较难以解决
 
### GRU 单元
> 门控循环单元 ，改变了RNN 的隐藏层，能更好地捕捉深层次连接，改善梯度消失的问题 

* 简化GRU 单元
  * 以时间步从左到右进行计算的时候，在GRU单元中，存在一个新的变量为c ,称为记忆细胞，并提供了长期记忆能力
  * $c^{<t>}=a^{<t>}$,记忆细胞实际为在时间步上的激活值$a^{<t>}$
  * $\tilde{c}^{<t>} = tabh(W_c[c^{<t-1>},x^{<t>}]+b_c)$  在每一个时间步，给定一个$\tilde{c}^{<t>},用来代替原本的记忆细胞c^{<t>}$
  * $\Gamma_u = \sigma(W_u[c^{t-1},x^{t}]+b_u)$ 代表更新门，值的范围0~1之间，用来决定是否对当前时间步的记忆细胞用候选值替换
  * $c^{<t>}=\Gamma*\tilde{c}^{<t>}+(1-\Gamma_u)*c^{<t-1>}$
  * $c^{<t>},\tilde{c}^{t},\Gamma_u$ 有相同的维度

![iamge](https://img-blog.csdn.net/20180304101019839?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* 完整的GRU单元
  * 完整的GRU单元还存在另外一个门，决定每个时间步的候选值
  * $\tilde{c}^{<t>} = tabh(W_c[\Gamma_r*c^{<t-1>},x^{<t>}]+b_c)$ 
  * $\Gamma_u = \sigma(W_u[c^{t-1},x^{t}]+b_u)$
  * $\Gamma_r = \sigma(W_r[c^{t-1},x^{t}]+b_r)$
  * $c^{<t>}=\Gamma*\tilde{c}^{<t>}+(1-\Gamma_u)*c^{<t-1>}$
  * $a^{<t>} = c^{<t>}$
  
  ### LSTM (long short term memory unit)
  > 长短期记忆，对捕捉序列中更深层次的联系要比GRU更加有效  
  LSTM 中 使用了单独的更新门$\Gamma_u$和遗忘门$\Gamma_f$,以及一个输出门$\Gamma_o$  
  遗忘门$\Gamma_f$代替$(1-\Gamma_u)$

  公式
* $\tilde{c}^{<t>} = tabh(W_c[\Gamma_r*c^{<t-1>},x^{<t>}]+b_c)$
* $\Gamma_u = \sigma(W_u[c^{t-1},x^{t}]+b_u)$
* $\Gamma_f = \sigma(W_f[c^{t-1},x^{t}]+b_f)$
* $\Gamma_o = \sigma(W_o[c^{t-1},x^{t}]+b_o)$
* $c^{<t>}=\Gamma*\tilde{c}^{<t>}+\Gamma_f*c^{<t-1>}$
* $a^{<t>} = \Gamma_o*tanhc^{<t>}$ 
  

![image](https://img-blog.csdn.net/20180304104125475?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

> 有时也可以偷窥一下$c^{<t-1>}$的值，叫做窥视孔连接，代表门的值不止取决于$a^{<t-1>}和x^{<t>}$也取决于上一个记忆细胞$c^{<t-1>}$  
>  例子 
> 假如有一个100维的隐藏记忆细胞单元，然后第50个$c^{<t-1>}$的元素只会影响到第50个元素对应的那个门，所以关系是一对一的

### 双向神经网络 双向RNN  BRNN 
前提： 单向RNN单元或者GRU单元或者LSTM单元，对一些序列的某个特征值的判断只能依靠序列的前面部分的特征值进行计算判断，有的时候需要序列后面部分的特征进行判断
* 在BRNN中，不仅有从左向右的前向连接层，还存在一个从右向左的反向连接层
![image](https://img-blog.csdn.net/20180304110051204?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
预测输出值
$\hat{y}^{<t> = g(W_y[\overrightarrow{a}^{<t>},\overleftarrow{a}^{<t>}]+b_y)}$  

* 缺点 ： 需要整个完整的输入序列，才能预测任意位置

### Deep RNN 
![image](https://img-blog.csdn.net/20180304112306924?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
> 某一层的激活值  
$a^{[2]<3>} = g(W_a^{[2]}[a^{[1]<3>},a^{[2]<2>}]+b_2^{[2]})$

> 对于像左边这样标准的神经网络，你可能见过很深的网络，甚至于100层深，而对于RNN来说，有三层就已经不少了。由于时间的维度，RNN网络会变得相当大，即使只有很少的几层，很少会看到这种网络堆叠到100层。但有一种会容易见到，就是在每一个上面堆叠循环层，把这里的输出去掉（上图编号1所示），然后换成一些深的层，这些层并不水平连接，只是一个深层的网络，然后用来预测$y^{<i>}$