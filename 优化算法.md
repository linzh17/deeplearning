# 深度学习优化算法 


## Mini-batch 梯度下降
> 对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，如有500万或5000万的训练数据，处理速度就会比较慢  
> 但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为Mini-batch。
\

$x=[x^{(1)}....x^{m}]$  
e.g. 假设有500万个数据 ,则每1000个数据为一个mini-batch ,则有5000个mini-batch  
将一个mini-batch 表示为$x^{\{i\}}$

>对于普通的梯度下降法，一个epoch只能进行一次梯度下降；而对于Mini-batch梯度下降法，一个epoch可以进行Mini-batch的个数次梯度下降。 

## 理解mini-batch 
>设mini-batch 大小为size  
if  size = m mini-batch 实际上为batch梯度下降  
if size = 1 mini-batch 实际上为随机梯度下降 SGD
* batch梯度下降： 
对所有m个训练样本执行一次梯度下降，每一次迭代时间较长；  
Cost function 总是向减小的方向下降。 

* 随机梯度下降：   
对每一个训练样本执行一次梯度下降，但是丢失了向量化带来的计算加速；  
Cost function总体的趋势向最小值的方向下降，但是无法到达全局最小值点，呈现波动的形式。 

* Mini-batch梯度下降：   
也呈波动的下降,最终趋势为下降  
选择一个1<size<m1<size<m 的合适的size进行Mini-batch梯度下降，可以实现快速学习，也应用了向量化带来的好处。  
Cost function的下降处于前两者之间  

### mini-batch 的大小选择 

* 如果训练样本的大小比较小时，如m⩽2000m⩽2000时 —— 选择batch梯度下降法；  
* 如果训练样本的大小比较大时，典型的大小为：   
$2^6,2^7.....,2^10$
*  Mini-batch的大小要符合CPU/GPU内存  不然算法表现较差

![image](https://img-blog.csdn.net/20171011095046354?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)  

## 指数加权平均  
> 指数加权公式 $v_t = \beta*v_{t-1}+(1-\beta)\theta_t$   
> 以每天的温度为例子 ,$v_t$代表t天的平均温度值,$\theta_t$代表第t天的温度值  
> $\beta$ 代表可调节的超参数值  

$\beta$=0.9的例子  
![image](https://pic2.zhimg.com/80/v2-d867aab342a51b94bf9ba54302138c9d_hd.jpg)  
化简得到的表达式  
![image](https://pic4.zhimg.com/80/v2-ab8fb1f005f9a0f17ff8d040ff0e276f_hd.jpg)  
> 通过上面表达式 可以看到  
> 本质就是以指数式递减加权的移动平均。各数值的加权而随时间而指数式递减，越近期的数据加权越重，但较旧的数据也给予一定的加权   
> 上式中所有θ前面的系数相加起来为1或者接近于1 称之为偏差修正  
> 当$(1-\delta)^{1/\delta} = \frac{1}{e}$  
>  $\beta=0.9$  
> 即 $0.9^10 \approx 0.35 \approx \frac{1}{e}$ 相当于大约十天后 系数的峰值下降到原来的1/e  
> 我们可以看到指数加权平均的求解过程实际上是一个递推的过程，那么这样就会有一个非常大的好处，每当我要求从0到某一时刻（n）的平均值的时候，我并不需要像普通求解平均值的作为，保留所有的时刻值，类和然后除以n。  
> 而是只需要保留0-(n-1)时刻的平均值和n时刻的温度值即可。也就是每次只需要保留常数值，然后进行运算即可，这对于深度学习中的海量数据来说，是一个很好的减少内存和空间的做法。  

### 指数加权平均实现
> v0=0  
> v1=βv0+(1−β)θ1  
> v2=βv1+(1−β)θ2  
> v3=βv2+(1−β)θ3…

### 指数加权平均的偏差修正  
>解决初期平均值的偏差  
![image](https://img-blog.csdn.net/20171011110513055?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)  
> 上图中,紫色线为起点较低,这就是初期平均值的偏差,远小与平均值 ,原因由公式 实现的公式可以发现,一开始的值会远小于实际值  

修正的方法  
使用$\frac{v_t}{1-\beta^t}$   
偏差修正得到了绿色的曲线,在开始的时候可以比紫色曲线更好的计算评价今年的效果,随着t的增大,$\beta^t$接近与0,绿色和紫色的曲线逐渐重合