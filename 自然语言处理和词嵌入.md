# 自然语言处理和词嵌入

## 词嵌入
> 使用one-hot向量表示词汇，会将每个词孤立起来，每个词的距离都一样，内积都为0，无法获取词  
> 与词之间的相似性和关联性  
> 这里引入词嵌入  
> 用不同的特征表达一个词汇，不同的单词在各个特征上都有不同的值，如图
![image](https://img-blog.csdn.net/20180304204727781?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
> 通过词嵌入可以容易地表示出词和词之间的相似性，用t-snes算法将高维的词向量映射到2维空间上，  
> 对词向量进行可视化
![image](https://img-blog.csdn.net/20180304204727781?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

## 词嵌入的使用
> 名字实体识别的例子：  

> 如下面的一个句子中名字实体的定位识别问题，假如我们有一个比较小的数据集，可能不包含durain（榴莲）和cultivator（培育家）这样的词汇，那么我们就很难从包含这两个词汇的句子中识别名字实体。但是如果我们从网上的其他地方获取了一个学习好的word Embedding，它将告诉我们榴莲是一种水果，并且培育家和农民相似，那么我们就有可能从我们少量的训练集中，归纳出没有见过的词汇中的名字实体。
![image](https://img-blog.csdn.net/20180304212549785?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
> 词嵌入的迁移学习：

> 有了词嵌入，就可以让我们能够使用迁移学习，通过网上大量的无标签的文本中学习到的知识，应用到我们少量文本训练集的任务中。下面是做词嵌入迁移学习的步骤：  

* 第一步：从大量的文本集合中学习word Embeddings（1-100B words），或者从网上下载预训练好的词嵌入模型；
* 第二步：将词嵌入模型迁移到我们小训练集的新任务上；比如说用这个300维的词嵌入来表示你的单词。这样做的一个好处就是你可以用更低维度的特征向量代替原来的10000维的one-hot向量，现在你可以用一个300维更加紧凑的向量
* 第三步：可选，使用我们新的标记数据对词嵌入模型继续进行微调。实际中，只有这个第二步中有很大的数据集你才会这样做


## 词嵌入的特性 
> 词嵌入可以实现类比推理  
> 比如说用这个300维的词嵌入来表示你的单词。这样做的一个好处就是你可以用更低维度的特征向量代替原来的10000维的one-hot向量，现在你可以用一个300维更加紧凑的向量  
> 
![image](https://img-blog.csdn.net/20180304221113224?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

> 计算词与词之间的相似度  

![image](https://img-blog.csdn.net/20180304222504196?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

$e_{man}-e_{woman}\approx e_{king}-e_{?}$  
找到一个一个词满足上述等式
则相当于寻找下面两个结果的向量之间的最大相似度：   
$max   sim(e_{?},e_{king}-e{man}+e_{woman})$  
可以使用的相似度函数 ，可以使用余弦相似度函数   
$sim(u,v) = \frac{u^Tv}{||u||_2||v||_2}$  
或者欧式距离：  
$|||u-v||^2$

## 嵌入矩阵 
> 在我们要对一个词汇表学习词嵌入模型时，实质上就是要学习这个词汇表对应的一个嵌入矩阵E。当我们学习好了这样一个嵌入矩阵后，通过嵌入矩阵与对应词的one-hot向量相乘，则可得到该词汇的embedding向量，如下图所示：  
![image](https://img-blog.csdn.net/20180304224313735?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

> 因为one-hot向量是一个维度非常高的向量，并且几乎所有元素都是0，所以矩阵向量相乘效率太低，因为我们要乘以一大堆的0。所以在实践中你会使用一个专门的函数来单独查找矩阵的某列，而不是用通常的矩阵乘法来做，但是在画示意图时（上图所示，即矩阵乘以one-hot向量示意图），这样写比较方便。但是例如在Keras中就有一个嵌入层，然后我们用这个嵌入层更有效地从嵌入矩阵中提取出你需要的列，而不是对矩阵进行很慢很复杂的乘法运算。

> 学习一个嵌入矩阵，需要随机初始化矩阵，然后使用梯度下降法学习嵌入矩阵的各个参数 

## 学习词嵌入  
> 早期的学习算法
* 通过前面几个单词，预测最后一个单词
  * 通过将每个单词的one-hot向量与嵌入矩阵相乘，得到相应的Embedding向量
  * 利用窗口控制影响预测结果的单词数量，并将窗口内单词的Embedding向量堆叠起来输入到神经网络中
  * 最后通过softmax层输出整个词汇表各个单词可能的概率
  * 整个模型的参数就是嵌入矩阵E，以及隐藏层和softmax层的参数$w^{[1]},b^{[1]},w^{[2]},b^{[2]}$
  * 利用反向传播算法进行梯度下降，最大化训练集似然函数，不断从语料库预测最后一个词的的输出
> 这个算法的激励下，apple和orange会学到很相似的嵌入，这样做能够让算法更好地拟合训练集，因为它有时看到的是orange juice，有时看到的是apple juice。如果你只用一个300维的特征向量来表示所有这些词，算法会发现要想最好地拟合训练集，就要使apple（苹果）、orange（橘子）、grape（葡萄）和pear（梨）等等，还有像durian（榴莲）这种很稀有的水果都拥有相似的特征向量。

* 我们将要预测的单词称为目标词，其是通过一些上下文推导预测出来的。对于不同的问题，上下文的大小和长度以及选择的方法有所不同。

  * 选取目标词之前的几个词；
  * 选取目标词前后的几个词；
  * 选取目标词前的一个词；
  * 选取目标词附近的一个词，（一种Skip-Gram模型的思想）。

## word2Vec 
### Skip-grams 
> 们要做的是抽取上下文和目标词配对，来构造一个监督学习问题。上下文不一定总是目标单词之前离得最近的四个单词，或最近的个单词。我们要的做的是随机选一个词作为上下文词 

* 模型流程 
  * 使用一个大量词汇的词汇表，size=10000k
  * 构建基本的监督学习问题，构建上下文和目标词的映射关系 C-T
  * $O_c-->E(词嵌入矩阵)-->e_c=E*o_c(词嵌入向量)-->softmax层-->\hat{y}$
  * Softnmax层 $p(t|c)=\frac{e^{\Theta^T_t}e_c}{\sum^{10000}_{j=1}e^{\Theta^T_j}e_c}$
  * 损失函数：$L(\hat{y},y) = -\sum^{10000}_{i=1}y_ilog\hat{y_i}$
  * 通过反向传播梯度下降的训练过程，可以得到模型参数E和softmax的参数
* 存在的问题
  * 在上面的Softmax单元中，我们需要对所有10000个整个词汇表的词做求和计算，计算量庞大。
  * 简化方案：使用分级softmax分类器（相当于一个树型分类器，每个节点都是可能是一个二分类器），其计算复杂度是前面的log|v|级别。在构造分级softmax分类器时，一般常用的词会放在树的顶部位置，而不常用的词则会放在树的更深处，其并不是一个平衡的二叉树。
> 在文献中你会看到的方法是使用一个分级（hierarchical）的softmax分类器，意思就是说不是一下子就确定到底是属于10,000类中的哪一类。想象如果你有一个分类器（上图编号1所示），它告诉你目标词是在词汇表的前5000个中还是在词汇表的后5000个词中，假如这个二分类器告诉你这个词在前5000个词中（上图编号2所示），然后第二个分类器会告诉你这个词在词汇表的前2500个词中，或者在词汇表的第二组2500个词中，诸如此类，直到最终你找到一个词准确所在的分类器（上图编号3所示），那么就是这棵树的一个叶子节点。像这样有一个树形的分类器，意味着树上内部的每一个节点都可以是一个二分类器，比如逻辑回归分类器，所以你不需要再为单次分类，对词汇表中所有的10,000个词求和了。实际上用这样的分类树，计算成本与词汇表大小的对数成正比（上图编号4所示），而不是词汇表大小的线性函数，这个就叫做分级softmax分类器。

> Word2Vec模型，Skip-Gram只是其中的一个，另一个叫做CBOW，即连续词袋模型（Continuous Bag-Of-Words Model），它获得中间词两边的的上下文，然后用周围的词去预测中间的词，这个模型也很有效，也有一些优点和缺点。

> 总结下：CBOW是从原始语句推测目标字词；而Skip-Gram正好相反，是从目标字词推测出原始语句。CBOW对小型数据库比较合适

* 如何采样上下文 
  * 对语料库均匀且随机地采样：使得如the、of、a等这样的一些词会出现的相当频繁，导致上下文和目标词对经常出现这类词汇，但我们想要的目标词却很少出现
  * 采用了不同的分级(启发)来平衡更常见的词和不那么常见的词  

## 负采样 
> Skip-grams模型可以帮助我们构造一个监督学习任务 缺点就是softmax计算的时间复杂度较高。  
> 负采样，它能做到与你刚才看到的Skip-Gram模型相似的事情，但是用了一个更加有效的学习算法

* 定义一个新的学习问题：预测两个词之间是否是上下文-目标词对，如果是词对，则学习的目标为1；否则为0 
* 使用k次相同的上下文，随机选择不同的目标词，并对相应的词对进行正负样本的标记，生成训练集。
* 建议：小数据集，k=5~20；大数据集，k=2~5
* 学习从x映射到y的监督学习模型
  * 采用logistic回归模型
  * $P(y=1|c,t)=\sigma(\Theta^T_te_c)$
  * 每个正样本均有k个对应的负样本。在训练的过程中，对于每个上下文词，我们就有对应的k+1个分类器。
  * ![image](https://img-blog.csdn.net/20180305103252362?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
  * 相比与Skip-grams模型，负采样不再使用一个具有词汇表大小时间复杂度高的庞大维度的Softmax，而是将其转换为词汇表大小个二分类问题。每个二分类任务都很容易解决，因为每个的训练样本均是1个正样本，外加k个负样本。
* 如何选择负样本
  * 在选定了上下文（Content）后，在确定正样本的情况下，我们还需要选择k个负样本以训练每个上下文的分类器。
  * 通过单词出现的频率进行采样：导致一些类似a、the、of等词的频率较高；
  * 均匀随机地抽取负样本：没有很好的代表性；
  * (推荐方法)$P(wi)=\frac{f(w_i)^{\frac{3}{4}}}{\sum^{10000}_{j=1}f(w_j)^{\frac{3}{4}}}$ 即不用频率分布，也不用均匀分布，而采用的是对词频的3/4除以词频3/4整体的和进行采样的。其中，f(wj)是语料库中观察到的某个词的词频。

## glove 词向量
> glove 词向量是另外一种计算词嵌入的模型。更为简单  
> GloVe算法做的就是使上下文和目标词关系开始明确化。假定$X_{ij}$是单词在单词上下文中出现的次数  
> 若上下午和目标词的范围定在左右各若干词的化，就会有一种对称关系    
> 那么就是$X_{ij}$一个能够获取单词和单词出现位置相近时或是彼此接近的频率的计数器。  
> GloVe模型做的就是进行优化，我们将他们之间的差距进行最小化处理：
$minimize\sum^{100000}_{i=1}\sum^{10000}_{j=1}f(X_{ij})(\theta^T_i e_j+b_i+b_j-logX_{ij})^2$
* 其中，因为当$X_{ij}$为0时，$logX_{ij}$便没有意义，所以添加$f(X_{ij})$的加权项，当$X_{ij}=0$时，$f(X_{ij})$，另外$f(X_{ij})$对于一些频繁词和不频繁词有着启发式的平衡作用；
* 另外，$\Theta^T_i e_j$这一项中，$\Theta^T_i$和$e_j$都是需要学习的参数，在这个目标算法中二者是对称的关系，所以我们可以一致地初始化$\Theta$和e，然后用梯度下降来最小化输出，在处理完所有词后，直接取二者的平均值作为词嵌入向量：$e^{final}_w=\frac{e_w+\Theta_w}{2}$，这与前面的算法有所不同。

### 词嵌入的特征化
> 通过上面的很多算法得到的词嵌入向量，我们无法保证词嵌入向量的每个独立分量是能够让我们理解的。我们能够确定是每个分量是和我们所想的一些特征是有关联的，其可能是一些我们能够理解的特征的组合而构成的一个组合分量。使用上面的GloVe模型，从线性代数的角度解释如下：
> $\Theta^T_i e_j =\Theta^T_iA^TA^{-T} e_j=(A{-T}e_j)$
![image](https://img-blog.csdn.net/20180305112354164?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


## 情感分类  
### 平均值或和的模型
* 获取一个训练好的词嵌入矩阵E；
* 得到每个词的词嵌入向量，并对所有的词向量做平均或者求和
* 得到每个词的词嵌入向量，并对所有的词向量做平均或者求和
* 缺点：没有考虑词序，可能会导致多数的积极词汇削弱前面消极词汇的影响，从而造成错误的预测。

![image](https://img-blog.csdn.net/20180305113822217?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### RNN 模型
* 获取一个训练好的词嵌入矩阵E
* 得到每个词的词嵌入向量，输入到many-to-one的RNN模型中
* 通过最后的softmax分类器，得到最后的输出
* 优点：考虑了词序，效果好很多
![image](https://img-blog.csdn.net/20180305114242100?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### 词嵌入消除偏见
> 以一些语料库中学习到的词嵌入向量，会发现学习到的词向量存在一些具有性别、种族等偏见，这反映了人们在历史的写作中存在的这种社会偏见

* 消除偏见
  * 定义偏见的方向：如性别   
    * 对大量性别相对的词汇进行相减并求平均：ehe−eshe、emale−efemale、⋯ehe−eshe、emale−efemale、⋯；
    *   通过平均后的向量，则可以得到一个或多个偏见趋势相关的维度，以及大量不相关的维度(图中bias维度为偏见维度，而无偏见趋势为299D的子空间)
  * 中和化：对每一个定义不明确的词汇，进行偏见的处理，如像doctor、babysitter这类词；通过减小这些词汇在得到的偏见趋势维度上值的大小；(减少或是消除他们的性别歧视趋势的成分，也就是说减少他们在这个水平方向上的距离（例如图中doctor的投影)，所以这就是第二个中和步)
  * 均衡：将如gradmother和gradfather这种对称词对调整至babysitter这类词汇平衡的位置上，使babysitter这类词汇处于一个中立的位置，进而消除偏见
  * 怎样才能够决定哪个词是中立的
    * 训练一个分类器来尝试解决哪些词是有明确定义的,一个线性分类器能够告诉你哪些词能够通过中和步来预测这个偏见趋势
![image](https://img-blog.csdn.net/20180305120911602?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS29hbGFfVHJlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

